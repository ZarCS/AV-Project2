{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c2a04",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-20T06:06:46.384147Z",
     "iopub.status.busy": "2024-11-20T06:06:46.383826Z",
     "iopub.status.idle": "2024-11-20T06:07:03.706263Z",
     "shell.execute_reply": "2024-11-20T06:07:03.705076Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 17.329011,
     "end_time": "2024-11-20T06:07:03.709011",
     "exception": false,
     "start_time": "2024-11-20T06:06:46.380000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install imgaug\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2d0fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T06:07:03.715346Z",
     "iopub.status.busy": "2024-11-20T06:07:03.715027Z",
     "iopub.status.idle": "2024-11-20T06:07:03.719259Z",
     "shell.execute_reply": "2024-11-20T06:07:03.718471Z"
    },
    "papermill": {
     "duration": 0.009368,
     "end_time": "2024-11-20T06:07:03.721066",
     "exception": false,
     "start_time": "2024-11-20T06:07:03.711698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/anti-noise-fgvr-v2')\n",
    "sys.path.append('/kaggle/input/anti-noise-fgvr-v2/example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f016ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T06:07:03.726475Z",
     "iopub.status.busy": "2024-11-20T06:07:03.726210Z",
     "iopub.status.idle": "2024-11-20T06:07:06.946665Z",
     "shell.execute_reply": "2024-11-20T06:07:06.945756Z"
    },
    "papermill": {
     "duration": 3.225352,
     "end_time": "2024-11-20T06:07:06.948606",
     "exception": false,
     "start_time": "2024-11-20T06:07:03.723254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of GPUs: 1\n",
      "Current GPU device: 0\n",
      "Current GPU name: Tesla P100-PCIE-16GB\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    print(\"CUDA is available!\")  \n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")  \n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")  \n",
    "    print(f\"Current GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")  \n",
    "else:  \n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc55cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T06:07:06.955079Z",
     "iopub.status.busy": "2024-11-20T06:07:06.954701Z",
     "iopub.status.idle": "2024-11-20T17:23:09.910639Z",
     "shell.execute_reply": "2024-11-20T17:23:09.909514Z"
    },
    "papermill": {
     "duration": 40562.961681,
     "end_time": "2024-11-20T17:23:09.912788",
     "exception": false,
     "start_time": "2024-11-20T06:07:06.951107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "==> Preparing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 544MB/s]\n",
      "/tmp/ipykernel_23/491003624.py:312: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/kaggle/input/pmal-checkpoint-model/' + store_name + '/checkpoint.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class '__main__.Anti_Noise_Decoder'>\n",
      "25\n",
      "100.0\n",
      "\n",
      "Epoch: 25\n",
      "Step: 0 | Loss1: 0.209 | Loss2: 0.18278 | Loss3: 0.09784 | Loss_Gen: 0.11495 |Loss_ORI: 0.26104 | Loss: 0.750 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.250 | Loss2: 0.13021 | Loss3: 0.09568 | Loss_Gen: 0.15285 |Loss_ORI: 0.20426 | Loss: 0.681 | Acc: 98.775% (403/408)\n",
      "Step: 100 | Loss1: 0.249 | Loss2: 0.12074 | Loss3: 0.10584 | Loss_Gen: 0.14913 |Loss_ORI: 0.20909 | Loss: 0.685 | Acc: 98.886% (799/808)\n",
      "Step: 150 | Loss1: 0.252 | Loss2: 0.11478 | Loss3: 0.10293 | Loss_Gen: 0.15061 |Loss_ORI: 0.21087 | Loss: 0.680 | Acc: 98.013% (1184/1208)\n",
      "Step: 200 | Loss1: 0.250 | Loss2: 0.11119 | Loss3: 0.10501 | Loss_Gen: 0.15045 |Loss_ORI: 0.21012 | Loss: 0.676 | Acc: 97.948% (1575/1608)\n",
      "Step: 250 | Loss1: 0.246 | Loss2: 0.10810 | Loss3: 0.09994 | Loss_Gen: 0.15047 |Loss_ORI: 0.20282 | Loss: 0.657 | Acc: 97.958% (1967/2008)\n",
      "Step: 300 | Loss1: 0.244 | Loss2: 0.10479 | Loss3: 0.09814 | Loss_Gen: 0.15044 |Loss_ORI: 0.20021 | Loss: 0.647 | Acc: 97.924% (2358/2408)\n",
      "Step: 350 | Loss1: 0.242 | Loss2: 0.10403 | Loss3: 0.09656 | Loss_Gen: 0.14964 |Loss_ORI: 0.19820 | Loss: 0.641 | Acc: 98.077% (2754/2808)\n",
      "Step: 400 | Loss1: 0.242 | Loss2: 0.10393 | Loss3: 0.09784 | Loss_Gen: 0.14947 |Loss_ORI: 0.20079 | Loss: 0.645 | Acc: 98.005% (3144/3208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/491003624.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | Loss: 0.036 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.062 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.058 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 26\n",
      "Step: 0 | Loss1: 0.215 | Loss2: 0.06237 | Loss3: 0.05075 | Loss_Gen: 0.17560 |Loss_ORI: 0.10759 | Loss: 0.436 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.221 | Loss2: 0.09083 | Loss3: 0.09455 | Loss_Gen: 0.14633 |Loss_ORI: 0.19385 | Loss: 0.600 | Acc: 98.775% (403/408)\n",
      "Step: 100 | Loss1: 0.233 | Loss2: 0.10266 | Loss3: 0.10276 | Loss_Gen: 0.14622 |Loss_ORI: 0.20904 | Loss: 0.648 | Acc: 98.144% (793/808)\n",
      "Step: 150 | Loss1: 0.236 | Loss2: 0.09909 | Loss3: 0.09908 | Loss_Gen: 0.14767 |Loss_ORI: 0.20464 | Loss: 0.639 | Acc: 98.427% (1189/1208)\n",
      "Step: 200 | Loss1: 0.234 | Loss2: 0.09839 | Loss3: 0.09686 | Loss_Gen: 0.14840 |Loss_ORI: 0.19914 | Loss: 0.629 | Acc: 98.507% (1584/1608)\n",
      "Step: 250 | Loss1: 0.233 | Loss2: 0.09899 | Loss3: 0.09417 | Loss_Gen: 0.14896 |Loss_ORI: 0.19692 | Loss: 0.623 | Acc: 98.406% (1976/2008)\n",
      "Step: 300 | Loss1: 0.232 | Loss2: 0.10008 | Loss3: 0.09353 | Loss_Gen: 0.14795 |Loss_ORI: 0.19595 | Loss: 0.621 | Acc: 98.339% (2368/2408)\n",
      "Step: 350 | Loss1: 0.232 | Loss2: 0.09797 | Loss3: 0.09319 | Loss_Gen: 0.14872 |Loss_ORI: 0.19249 | Loss: 0.615 | Acc: 98.291% (2760/2808)\n",
      "Step: 400 | Loss1: 0.233 | Loss2: 0.09814 | Loss3: 0.09300 | Loss_Gen: 0.14812 |Loss_ORI: 0.19255 | Loss: 0.617 | Acc: 98.192% (3150/3208)\n",
      "Step: 0 | Loss: 0.054 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.050 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.056 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 27\n",
      "Step: 0 | Loss1: 0.275 | Loss2: 0.21913 | Loss3: 0.12401 | Loss_Gen: 0.15100 |Loss_ORI: 0.18686 | Loss: 0.805 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.241 | Loss2: 0.11525 | Loss3: 0.10158 | Loss_Gen: 0.15083 |Loss_ORI: 0.22047 | Loss: 0.678 | Acc: 98.039% (400/408)\n",
      "Step: 100 | Loss1: 0.230 | Loss2: 0.10587 | Loss3: 0.09415 | Loss_Gen: 0.14872 |Loss_ORI: 0.20531 | Loss: 0.636 | Acc: 97.896% (791/808)\n",
      "Step: 150 | Loss1: 0.228 | Loss2: 0.10406 | Loss3: 0.09268 | Loss_Gen: 0.14625 |Loss_ORI: 0.20255 | Loss: 0.628 | Acc: 97.599% (1179/1208)\n",
      "Step: 200 | Loss1: 0.231 | Loss2: 0.10353 | Loss3: 0.09265 | Loss_Gen: 0.14694 |Loss_ORI: 0.20315 | Loss: 0.630 | Acc: 97.637% (1570/1608)\n",
      "Step: 250 | Loss1: 0.234 | Loss2: 0.10298 | Loss3: 0.09358 | Loss_Gen: 0.14662 |Loss_ORI: 0.20390 | Loss: 0.634 | Acc: 97.610% (1960/2008)\n",
      "Step: 300 | Loss1: 0.236 | Loss2: 0.10233 | Loss3: 0.09751 | Loss_Gen: 0.14654 |Loss_ORI: 0.20468 | Loss: 0.641 | Acc: 97.716% (2353/2408)\n",
      "Step: 350 | Loss1: 0.236 | Loss2: 0.10315 | Loss3: 0.09713 | Loss_Gen: 0.14651 |Loss_ORI: 0.20156 | Loss: 0.638 | Acc: 98.006% (2752/2808)\n",
      "Step: 400 | Loss1: 0.235 | Loss2: 0.10376 | Loss3: 0.09681 | Loss_Gen: 0.14700 |Loss_ORI: 0.20118 | Loss: 0.637 | Acc: 98.005% (3144/3208)\n",
      "Step: 0 | Loss: 0.048 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.047 |Combined Acc: 99.720% (356/357)\n",
      "Step: 100 | Loss: 0.048 |Combined Acc: 99.717% (705/707)\n",
      "\n",
      "Epoch: 28\n",
      "Step: 0 | Loss1: 0.173 | Loss2: 0.08749 | Loss3: 0.08792 | Loss_Gen: 0.12366 |Loss_ORI: 0.15944 | Loss: 0.507 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.217 | Loss2: 0.09107 | Loss3: 0.08697 | Loss_Gen: 0.14442 |Loss_ORI: 0.18182 | Loss: 0.577 | Acc: 98.529% (402/408)\n",
      "Step: 100 | Loss1: 0.226 | Loss2: 0.08960 | Loss3: 0.08532 | Loss_Gen: 0.14455 |Loss_ORI: 0.17742 | Loss: 0.578 | Acc: 98.639% (797/808)\n",
      "Step: 150 | Loss1: 0.225 | Loss2: 0.09268 | Loss3: 0.09058 | Loss_Gen: 0.14596 |Loss_ORI: 0.18488 | Loss: 0.593 | Acc: 98.427% (1189/1208)\n",
      "Step: 200 | Loss1: 0.227 | Loss2: 0.09490 | Loss3: 0.09433 | Loss_Gen: 0.14540 |Loss_ORI: 0.19201 | Loss: 0.608 | Acc: 98.259% (1580/1608)\n",
      "Step: 250 | Loss1: 0.235 | Loss2: 0.10155 | Loss3: 0.10139 | Loss_Gen: 0.14462 |Loss_ORI: 0.20561 | Loss: 0.643 | Acc: 98.008% (1968/2008)\n",
      "Step: 300 | Loss1: 0.231 | Loss2: 0.09979 | Loss3: 0.09740 | Loss_Gen: 0.14402 |Loss_ORI: 0.20027 | Loss: 0.629 | Acc: 98.048% (2361/2408)\n",
      "Step: 350 | Loss1: 0.231 | Loss2: 0.09897 | Loss3: 0.09645 | Loss_Gen: 0.14449 |Loss_ORI: 0.19621 | Loss: 0.623 | Acc: 98.184% (2757/2808)\n",
      "Step: 400 | Loss1: 0.230 | Loss2: 0.09802 | Loss3: 0.09495 | Loss_Gen: 0.14415 |Loss_ORI: 0.19523 | Loss: 0.618 | Acc: 98.223% (3151/3208)\n",
      "Step: 0 | Loss: 0.065 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.055 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.056 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 29\n",
      "Step: 0 | Loss1: 0.151 | Loss2: 0.04853 | Loss3: 0.04041 | Loss_Gen: 0.15848 |Loss_ORI: 0.08434 | Loss: 0.324 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.207 | Loss2: 0.09519 | Loss3: 0.08861 | Loss_Gen: 0.14055 |Loss_ORI: 0.17879 | Loss: 0.570 | Acc: 98.529% (402/408)\n",
      "Step: 100 | Loss1: 0.221 | Loss2: 0.09847 | Loss3: 0.09162 | Loss_Gen: 0.14113 |Loss_ORI: 0.18492 | Loss: 0.596 | Acc: 98.639% (797/808)\n",
      "Step: 150 | Loss1: 0.219 | Loss2: 0.09495 | Loss3: 0.09011 | Loss_Gen: 0.14279 |Loss_ORI: 0.17951 | Loss: 0.584 | Acc: 98.758% (1193/1208)\n",
      "Step: 200 | Loss1: 0.219 | Loss2: 0.09358 | Loss3: 0.08732 | Loss_Gen: 0.14338 |Loss_ORI: 0.17695 | Loss: 0.577 | Acc: 98.818% (1589/1608)\n",
      "Step: 250 | Loss1: 0.225 | Loss2: 0.09694 | Loss3: 0.08900 | Loss_Gen: 0.14365 |Loss_ORI: 0.18392 | Loss: 0.594 | Acc: 98.855% (1985/2008)\n",
      "Step: 300 | Loss1: 0.227 | Loss2: 0.09726 | Loss3: 0.09208 | Loss_Gen: 0.14425 |Loss_ORI: 0.18896 | Loss: 0.605 | Acc: 98.796% (2379/2408)\n",
      "Step: 350 | Loss1: 0.226 | Loss2: 0.09680 | Loss3: 0.09091 | Loss_Gen: 0.14320 |Loss_ORI: 0.18781 | Loss: 0.602 | Acc: 98.825% (2775/2808)\n",
      "Step: 400 | Loss1: 0.224 | Loss2: 0.09417 | Loss3: 0.08833 | Loss_Gen: 0.14317 |Loss_ORI: 0.18235 | Loss: 0.588 | Acc: 98.940% (3174/3208)\n",
      "Step: 0 | Loss: 0.048 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.047 |Combined Acc: 99.720% (356/357)\n",
      "Step: 100 | Loss: 0.047 |Combined Acc: 99.859% (706/707)\n",
      "\n",
      "Epoch: 30\n",
      "Step: 0 | Loss1: 0.163 | Loss2: 0.11480 | Loss3: 0.07209 | Loss_Gen: 0.12146 |Loss_ORI: 0.15231 | Loss: 0.502 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.225 | Loss2: 0.11236 | Loss3: 0.11175 | Loss_Gen: 0.14225 |Loss_ORI: 0.18876 | Loss: 0.638 | Acc: 98.775% (403/408)\n",
      "Step: 100 | Loss1: 0.213 | Loss2: 0.09615 | Loss3: 0.09447 | Loss_Gen: 0.14434 |Loss_ORI: 0.17349 | Loss: 0.578 | Acc: 99.010% (800/808)\n",
      "Step: 150 | Loss1: 0.215 | Loss2: 0.09558 | Loss3: 0.08916 | Loss_Gen: 0.14404 |Loss_ORI: 0.17293 | Loss: 0.573 | Acc: 98.924% (1195/1208)\n",
      "Step: 200 | Loss1: 0.214 | Loss2: 0.09513 | Loss3: 0.08674 | Loss_Gen: 0.14221 |Loss_ORI: 0.16979 | Loss: 0.566 | Acc: 99.067% (1593/1608)\n",
      "Step: 250 | Loss1: 0.216 | Loss2: 0.09625 | Loss3: 0.08623 | Loss_Gen: 0.14039 |Loss_ORI: 0.17310 | Loss: 0.571 | Acc: 98.805% (1984/2008)\n",
      "Step: 300 | Loss1: 0.216 | Loss2: 0.09708 | Loss3: 0.08633 | Loss_Gen: 0.14024 |Loss_ORI: 0.17506 | Loss: 0.574 | Acc: 98.796% (2379/2408)\n",
      "Step: 350 | Loss1: 0.215 | Loss2: 0.09696 | Loss3: 0.08635 | Loss_Gen: 0.13976 |Loss_ORI: 0.17449 | Loss: 0.573 | Acc: 98.825% (2775/2808)\n",
      "Step: 400 | Loss1: 0.216 | Loss2: 0.09849 | Loss3: 0.08631 | Loss_Gen: 0.13950 |Loss_ORI: 0.17624 | Loss: 0.577 | Acc: 98.784% (3169/3208)\n",
      "Step: 0 | Loss: 0.043 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.052 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.051 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 31\n",
      "Step: 0 | Loss1: 0.173 | Loss2: 0.11543 | Loss3: 0.10170 | Loss_Gen: 0.10270 |Loss_ORI: 0.16105 | Loss: 0.551 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.217 | Loss2: 0.09303 | Loss3: 0.08023 | Loss_Gen: 0.13119 |Loss_ORI: 0.18269 | Loss: 0.573 | Acc: 98.775% (403/408)\n",
      "Step: 100 | Loss1: 0.225 | Loss2: 0.10159 | Loss3: 0.09203 | Loss_Gen: 0.13178 |Loss_ORI: 0.19900 | Loss: 0.618 | Acc: 97.896% (791/808)\n",
      "Step: 150 | Loss1: 0.220 | Loss2: 0.09880 | Loss3: 0.08711 | Loss_Gen: 0.13346 |Loss_ORI: 0.19188 | Loss: 0.598 | Acc: 98.096% (1185/1208)\n",
      "Step: 200 | Loss1: 0.215 | Loss2: 0.09706 | Loss3: 0.08542 | Loss_Gen: 0.13452 |Loss_ORI: 0.18360 | Loss: 0.581 | Acc: 98.197% (1579/1608)\n",
      "Step: 250 | Loss1: 0.213 | Loss2: 0.09671 | Loss3: 0.08430 | Loss_Gen: 0.13497 |Loss_ORI: 0.18146 | Loss: 0.575 | Acc: 98.207% (1972/2008)\n",
      "Step: 300 | Loss1: 0.212 | Loss2: 0.09765 | Loss3: 0.08510 | Loss_Gen: 0.13550 |Loss_ORI: 0.18218 | Loss: 0.577 | Acc: 98.007% (2360/2408)\n",
      "Step: 350 | Loss1: 0.212 | Loss2: 0.09592 | Loss3: 0.08414 | Loss_Gen: 0.13634 |Loss_ORI: 0.17930 | Loss: 0.571 | Acc: 98.077% (2754/2808)\n",
      "Step: 400 | Loss1: 0.216 | Loss2: 0.09638 | Loss3: 0.08547 | Loss_Gen: 0.13725 |Loss_ORI: 0.18153 | Loss: 0.580 | Acc: 98.192% (3150/3208)\n",
      "Step: 0 | Loss: 0.044 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.040 |Combined Acc: 99.720% (356/357)\n",
      "Step: 100 | Loss: 0.040 |Combined Acc: 99.717% (705/707)\n",
      "\n",
      "Epoch: 32\n",
      "Step: 0 | Loss1: 0.248 | Loss2: 0.07668 | Loss3: 0.08060 | Loss_Gen: 0.11982 |Loss_ORI: 0.13245 | Loss: 0.538 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.184 | Loss2: 0.07253 | Loss3: 0.06282 | Loss_Gen: 0.13908 |Loss_ORI: 0.13226 | Loss: 0.452 | Acc: 99.755% (407/408)\n",
      "Step: 100 | Loss1: 0.193 | Loss2: 0.07504 | Loss3: 0.06886 | Loss_Gen: 0.14207 |Loss_ORI: 0.14082 | Loss: 0.477 | Acc: 99.010% (800/808)\n",
      "Step: 150 | Loss1: 0.195 | Loss2: 0.07674 | Loss3: 0.07346 | Loss_Gen: 0.14148 |Loss_ORI: 0.14704 | Loss: 0.493 | Acc: 98.758% (1193/1208)\n",
      "Step: 200 | Loss1: 0.204 | Loss2: 0.08467 | Loss3: 0.07879 | Loss_Gen: 0.14114 |Loss_ORI: 0.16009 | Loss: 0.528 | Acc: 98.632% (1586/1608)\n",
      "Step: 250 | Loss1: 0.207 | Loss2: 0.08889 | Loss3: 0.08114 | Loss_Gen: 0.13879 |Loss_ORI: 0.16476 | Loss: 0.541 | Acc: 98.556% (1979/2008)\n",
      "Step: 300 | Loss1: 0.207 | Loss2: 0.08920 | Loss3: 0.08214 | Loss_Gen: 0.13861 |Loss_ORI: 0.16580 | Loss: 0.544 | Acc: 98.754% (2378/2408)\n",
      "Step: 350 | Loss1: 0.210 | Loss2: 0.08965 | Loss3: 0.08262 | Loss_Gen: 0.13806 |Loss_ORI: 0.17043 | Loss: 0.553 | Acc: 98.682% (2771/2808)\n",
      "Step: 400 | Loss1: 0.208 | Loss2: 0.08878 | Loss3: 0.08256 | Loss_Gen: 0.13753 |Loss_ORI: 0.16976 | Loss: 0.549 | Acc: 98.691% (3166/3208)\n",
      "Step: 0 | Loss: 0.032 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.039 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.041 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 33\n",
      "Step: 0 | Loss1: 0.207 | Loss2: 0.10964 | Loss3: 0.08700 | Loss_Gen: 0.11419 |Loss_ORI: 0.19859 | Loss: 0.602 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.196 | Loss2: 0.08640 | Loss3: 0.07186 | Loss_Gen: 0.13245 |Loss_ORI: 0.15693 | Loss: 0.512 | Acc: 98.284% (401/408)\n",
      "Step: 100 | Loss1: 0.201 | Loss2: 0.08500 | Loss3: 0.07686 | Loss_Gen: 0.13013 |Loss_ORI: 0.16559 | Loss: 0.528 | Acc: 98.267% (794/808)\n",
      "Step: 150 | Loss1: 0.198 | Loss2: 0.08890 | Loss3: 0.08044 | Loss_Gen: 0.13151 |Loss_ORI: 0.16569 | Loss: 0.533 | Acc: 98.427% (1189/1208)\n",
      "Step: 200 | Loss1: 0.202 | Loss2: 0.09148 | Loss3: 0.07957 | Loss_Gen: 0.13280 |Loss_ORI: 0.16528 | Loss: 0.538 | Acc: 98.632% (1586/1608)\n",
      "Step: 250 | Loss1: 0.198 | Loss2: 0.09146 | Loss3: 0.07958 | Loss_Gen: 0.13229 |Loss_ORI: 0.16619 | Loss: 0.536 | Acc: 98.655% (1981/2008)\n",
      "Step: 300 | Loss1: 0.199 | Loss2: 0.09147 | Loss3: 0.07939 | Loss_Gen: 0.13273 |Loss_ORI: 0.16645 | Loss: 0.536 | Acc: 98.713% (2377/2408)\n",
      "Step: 350 | Loss1: 0.199 | Loss2: 0.09094 | Loss3: 0.07879 | Loss_Gen: 0.13262 |Loss_ORI: 0.16585 | Loss: 0.535 | Acc: 98.754% (2773/2808)\n",
      "Step: 400 | Loss1: 0.201 | Loss2: 0.09127 | Loss3: 0.07894 | Loss_Gen: 0.13237 |Loss_ORI: 0.16638 | Loss: 0.537 | Acc: 98.847% (3171/3208)\n",
      "Step: 0 | Loss: 0.013 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.025 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.027 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 34\n",
      "Step: 0 | Loss1: 0.160 | Loss2: 0.06201 | Loss3: 0.06248 | Loss_Gen: 0.12962 |Loss_ORI: 0.12686 | Loss: 0.411 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.224 | Loss2: 0.09520 | Loss3: 0.09150 | Loss_Gen: 0.13013 |Loss_ORI: 0.18009 | Loss: 0.591 | Acc: 99.265% (405/408)\n",
      "Step: 100 | Loss1: 0.221 | Loss2: 0.09988 | Loss3: 0.09084 | Loss_Gen: 0.12988 |Loss_ORI: 0.18346 | Loss: 0.596 | Acc: 98.886% (799/808)\n",
      "Step: 150 | Loss1: 0.221 | Loss2: 0.09963 | Loss3: 0.08949 | Loss_Gen: 0.13210 |Loss_ORI: 0.18700 | Loss: 0.597 | Acc: 98.841% (1194/1208)\n",
      "Step: 200 | Loss1: 0.215 | Loss2: 0.09531 | Loss3: 0.08431 | Loss_Gen: 0.13120 |Loss_ORI: 0.17908 | Loss: 0.573 | Acc: 98.756% (1588/1608)\n",
      "Step: 250 | Loss1: 0.214 | Loss2: 0.09554 | Loss3: 0.08789 | Loss_Gen: 0.13056 |Loss_ORI: 0.18055 | Loss: 0.578 | Acc: 98.655% (1981/2008)\n",
      "Step: 300 | Loss1: 0.213 | Loss2: 0.09438 | Loss3: 0.08529 | Loss_Gen: 0.13020 |Loss_ORI: 0.17821 | Loss: 0.570 | Acc: 98.671% (2376/2408)\n",
      "Step: 350 | Loss1: 0.210 | Loss2: 0.09303 | Loss3: 0.08530 | Loss_Gen: 0.12999 |Loss_ORI: 0.17631 | Loss: 0.564 | Acc: 98.789% (2774/2808)\n",
      "Step: 400 | Loss1: 0.208 | Loss2: 0.09249 | Loss3: 0.08408 | Loss_Gen: 0.12957 |Loss_ORI: 0.17544 | Loss: 0.560 | Acc: 98.784% (3169/3208)\n",
      "Step: 0 | Loss: 0.039 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.036 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.038 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 35\n",
      "Step: 0 | Loss1: 0.157 | Loss2: 0.05447 | Loss3: 0.04911 | Loss_Gen: 0.14300 |Loss_ORI: 0.10224 | Loss: 0.362 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.171 | Loss2: 0.07921 | Loss3: 0.06545 | Loss_Gen: 0.12565 |Loss_ORI: 0.14171 | Loss: 0.458 | Acc: 99.510% (406/408)\n",
      "Step: 100 | Loss1: 0.183 | Loss2: 0.08532 | Loss3: 0.07647 | Loss_Gen: 0.12214 |Loss_ORI: 0.15964 | Loss: 0.505 | Acc: 98.886% (799/808)\n",
      "Step: 150 | Loss1: 0.187 | Loss2: 0.08871 | Loss3: 0.08213 | Loss_Gen: 0.12144 |Loss_ORI: 0.16440 | Loss: 0.522 | Acc: 99.007% (1196/1208)\n",
      "Step: 200 | Loss1: 0.194 | Loss2: 0.09079 | Loss3: 0.08336 | Loss_Gen: 0.12339 |Loss_ORI: 0.16713 | Loss: 0.535 | Acc: 98.818% (1589/1608)\n",
      "Step: 250 | Loss1: 0.192 | Loss2: 0.08920 | Loss3: 0.08205 | Loss_Gen: 0.12371 |Loss_ORI: 0.16375 | Loss: 0.527 | Acc: 98.904% (1986/2008)\n",
      "Step: 300 | Loss1: 0.187 | Loss2: 0.08734 | Loss3: 0.08061 | Loss_Gen: 0.12316 |Loss_ORI: 0.16043 | Loss: 0.516 | Acc: 99.003% (2384/2408)\n",
      "Step: 350 | Loss1: 0.186 | Loss2: 0.08720 | Loss3: 0.08165 | Loss_Gen: 0.12201 |Loss_ORI: 0.16151 | Loss: 0.516 | Acc: 98.967% (2779/2808)\n",
      "Step: 400 | Loss1: 0.186 | Loss2: 0.08976 | Loss3: 0.08200 | Loss_Gen: 0.12076 |Loss_ORI: 0.16395 | Loss: 0.522 | Acc: 99.002% (3176/3208)\n",
      "Step: 0 | Loss: 0.035 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.042 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.040 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 36\n",
      "Step: 0 | Loss1: 0.140 | Loss2: 0.07215 | Loss3: 0.05857 | Loss_Gen: 0.11024 |Loss_ORI: 0.11419 | Loss: 0.385 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.199 | Loss2: 0.10355 | Loss3: 0.09131 | Loss_Gen: 0.11516 |Loss_ORI: 0.19143 | Loss: 0.585 | Acc: 98.284% (401/408)\n",
      "Step: 100 | Loss1: 0.192 | Loss2: 0.09814 | Loss3: 0.08360 | Loss_Gen: 0.11743 |Loss_ORI: 0.17139 | Loss: 0.545 | Acc: 98.515% (796/808)\n",
      "Step: 150 | Loss1: 0.189 | Loss2: 0.09466 | Loss3: 0.08155 | Loss_Gen: 0.11815 |Loss_ORI: 0.16762 | Loss: 0.532 | Acc: 98.675% (1192/1208)\n",
      "Step: 200 | Loss1: 0.196 | Loss2: 0.09960 | Loss3: 0.09246 | Loss_Gen: 0.11761 |Loss_ORI: 0.17786 | Loss: 0.566 | Acc: 98.694% (1587/1608)\n",
      "Step: 250 | Loss1: 0.191 | Loss2: 0.09573 | Loss3: 0.08851 | Loss_Gen: 0.11754 |Loss_ORI: 0.17599 | Loss: 0.552 | Acc: 98.655% (1981/2008)\n",
      "Step: 300 | Loss1: 0.189 | Loss2: 0.09517 | Loss3: 0.08784 | Loss_Gen: 0.11707 |Loss_ORI: 0.17465 | Loss: 0.547 | Acc: 98.630% (2375/2408)\n",
      "Step: 350 | Loss1: 0.186 | Loss2: 0.09270 | Loss3: 0.08497 | Loss_Gen: 0.11568 |Loss_ORI: 0.17008 | Loss: 0.534 | Acc: 98.789% (2774/2808)\n",
      "Step: 400 | Loss1: 0.185 | Loss2: 0.09141 | Loss3: 0.08370 | Loss_Gen: 0.11487 |Loss_ORI: 0.16862 | Loss: 0.528 | Acc: 98.784% (3169/3208)\n",
      "Step: 0 | Loss: 0.020 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.038 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.034 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 37\n",
      "Step: 0 | Loss1: 0.178 | Loss2: 0.06584 | Loss3: 0.06424 | Loss_Gen: 0.12863 |Loss_ORI: 0.17424 | Loss: 0.482 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.193 | Loss2: 0.09293 | Loss3: 0.08822 | Loss_Gen: 0.10866 |Loss_ORI: 0.17572 | Loss: 0.550 | Acc: 99.510% (406/408)\n",
      "Step: 100 | Loss1: 0.195 | Loss2: 0.09004 | Loss3: 0.08898 | Loss_Gen: 0.10664 |Loss_ORI: 0.17914 | Loss: 0.553 | Acc: 98.886% (799/808)\n",
      "Step: 150 | Loss1: 0.188 | Loss2: 0.09648 | Loss3: 0.08574 | Loss_Gen: 0.10707 |Loss_ORI: 0.17783 | Loss: 0.548 | Acc: 98.675% (1192/1208)\n",
      "Step: 200 | Loss1: 0.180 | Loss2: 0.09256 | Loss3: 0.08208 | Loss_Gen: 0.10617 |Loss_ORI: 0.17113 | Loss: 0.526 | Acc: 98.694% (1587/1608)\n",
      "Step: 250 | Loss1: 0.179 | Loss2: 0.09513 | Loss3: 0.08287 | Loss_Gen: 0.10397 |Loss_ORI: 0.17290 | Loss: 0.530 | Acc: 98.855% (1985/2008)\n",
      "Step: 300 | Loss1: 0.176 | Loss2: 0.09295 | Loss3: 0.08152 | Loss_Gen: 0.10255 |Loss_ORI: 0.17068 | Loss: 0.521 | Acc: 98.796% (2379/2408)\n",
      "Step: 350 | Loss1: 0.175 | Loss2: 0.09378 | Loss3: 0.08137 | Loss_Gen: 0.10153 |Loss_ORI: 0.17019 | Loss: 0.520 | Acc: 98.825% (2775/2808)\n",
      "Step: 400 | Loss1: 0.171 | Loss2: 0.09184 | Loss3: 0.07914 | Loss_Gen: 0.10031 |Loss_ORI: 0.16583 | Loss: 0.507 | Acc: 98.847% (3171/3208)\n",
      "Step: 0 | Loss: 0.020 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.029 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.034 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 38\n",
      "Step: 0 | Loss1: 0.189 | Loss2: 0.10016 | Loss3: 0.07261 | Loss_Gen: 0.09374 |Loss_ORI: 0.15095 | Loss: 0.513 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.163 | Loss2: 0.10310 | Loss3: 0.08311 | Loss_Gen: 0.08364 |Loss_ORI: 0.17232 | Loss: 0.522 | Acc: 98.284% (401/408)\n",
      "Step: 100 | Loss1: 0.149 | Loss2: 0.09470 | Loss3: 0.07726 | Loss_Gen: 0.08448 |Loss_ORI: 0.16015 | Loss: 0.481 | Acc: 98.762% (798/808)\n",
      "Step: 150 | Loss1: 0.148 | Loss2: 0.09024 | Loss3: 0.07646 | Loss_Gen: 0.08464 |Loss_ORI: 0.15924 | Loss: 0.474 | Acc: 98.924% (1195/1208)\n",
      "Step: 200 | Loss1: 0.150 | Loss2: 0.09020 | Loss3: 0.07662 | Loss_Gen: 0.08505 |Loss_ORI: 0.15899 | Loss: 0.475 | Acc: 99.067% (1593/1608)\n",
      "Step: 250 | Loss1: 0.146 | Loss2: 0.08699 | Loss3: 0.07426 | Loss_Gen: 0.08395 |Loss_ORI: 0.15510 | Loss: 0.462 | Acc: 99.054% (1989/2008)\n",
      "Step: 300 | Loss1: 0.142 | Loss2: 0.08566 | Loss3: 0.07284 | Loss_Gen: 0.08276 |Loss_ORI: 0.15312 | Loss: 0.453 | Acc: 99.045% (2385/2408)\n",
      "Step: 350 | Loss1: 0.143 | Loss2: 0.08693 | Loss3: 0.07438 | Loss_Gen: 0.08184 |Loss_ORI: 0.15634 | Loss: 0.461 | Acc: 98.932% (2778/2808)\n",
      "Step: 400 | Loss1: 0.145 | Loss2: 0.08719 | Loss3: 0.07557 | Loss_Gen: 0.08148 |Loss_ORI: 0.15871 | Loss: 0.466 | Acc: 98.971% (3175/3208)\n",
      "Step: 0 | Loss: 0.033 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.033 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.032 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 39\n",
      "Step: 0 | Loss1: 0.096 | Loss2: 0.04890 | Loss3: 0.10897 | Loss_Gen: 0.08833 |Loss_ORI: 0.11142 | Loss: 0.366 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.130 | Loss2: 0.07955 | Loss3: 0.07290 | Loss_Gen: 0.07663 |Loss_ORI: 0.15541 | Loss: 0.438 | Acc: 99.020% (404/408)\n",
      "Step: 100 | Loss1: 0.130 | Loss2: 0.08063 | Loss3: 0.07115 | Loss_Gen: 0.07601 |Loss_ORI: 0.15013 | Loss: 0.432 | Acc: 99.010% (800/808)\n",
      "Step: 150 | Loss1: 0.131 | Loss2: 0.08116 | Loss3: 0.07226 | Loss_Gen: 0.07602 |Loss_ORI: 0.15236 | Loss: 0.436 | Acc: 99.172% (1198/1208)\n",
      "Step: 200 | Loss1: 0.135 | Loss2: 0.08383 | Loss3: 0.07444 | Loss_Gen: 0.07589 |Loss_ORI: 0.15398 | Loss: 0.447 | Acc: 99.254% (1596/1608)\n",
      "Step: 250 | Loss1: 0.133 | Loss2: 0.08123 | Loss3: 0.07297 | Loss_Gen: 0.07601 |Loss_ORI: 0.14781 | Loss: 0.435 | Acc: 99.402% (1996/2008)\n",
      "Step: 300 | Loss1: 0.134 | Loss2: 0.08098 | Loss3: 0.07518 | Loss_Gen: 0.07532 |Loss_ORI: 0.15112 | Loss: 0.441 | Acc: 99.336% (2392/2408)\n",
      "Step: 350 | Loss1: 0.137 | Loss2: 0.08160 | Loss3: 0.07585 | Loss_Gen: 0.07509 |Loss_ORI: 0.15278 | Loss: 0.447 | Acc: 99.288% (2788/2808)\n",
      "Step: 400 | Loss1: 0.137 | Loss2: 0.08202 | Loss3: 0.07523 | Loss_Gen: 0.07495 |Loss_ORI: 0.15543 | Loss: 0.450 | Acc: 99.127% (3180/3208)\n",
      "Step: 0 | Loss: 0.041 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.026 |Combined Acc: 99.720% (356/357)\n",
      "Step: 100 | Loss: 0.026 |Combined Acc: 99.859% (706/707)\n",
      "\n",
      "Epoch: 40\n",
      "Step: 0 | Loss1: 0.066 | Loss2: 0.03514 | Loss3: 0.02887 | Loss_Gen: 0.08141 |Loss_ORI: 0.07117 | Loss: 0.201 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.136 | Loss2: 0.08851 | Loss3: 0.08136 | Loss_Gen: 0.07216 |Loss_ORI: 0.15293 | Loss: 0.458 | Acc: 99.265% (405/408)\n",
      "Step: 100 | Loss1: 0.137 | Loss2: 0.08231 | Loss3: 0.07558 | Loss_Gen: 0.07184 |Loss_ORI: 0.14696 | Loss: 0.441 | Acc: 99.257% (802/808)\n",
      "Step: 150 | Loss1: 0.144 | Loss2: 0.08939 | Loss3: 0.07980 | Loss_Gen: 0.07134 |Loss_ORI: 0.16053 | Loss: 0.473 | Acc: 99.255% (1199/1208)\n",
      "Step: 200 | Loss1: 0.142 | Loss2: 0.08656 | Loss3: 0.07784 | Loss_Gen: 0.07087 |Loss_ORI: 0.15698 | Loss: 0.464 | Acc: 99.192% (1595/1608)\n",
      "Step: 250 | Loss1: 0.138 | Loss2: 0.08571 | Loss3: 0.07632 | Loss_Gen: 0.07054 |Loss_ORI: 0.15427 | Loss: 0.454 | Acc: 99.203% (1992/2008)\n",
      "Step: 300 | Loss1: 0.135 | Loss2: 0.08403 | Loss3: 0.07490 | Loss_Gen: 0.07001 |Loss_ORI: 0.15284 | Loss: 0.446 | Acc: 99.252% (2390/2408)\n",
      "Step: 350 | Loss1: 0.135 | Loss2: 0.08350 | Loss3: 0.07456 | Loss_Gen: 0.06952 |Loss_ORI: 0.15404 | Loss: 0.447 | Acc: 99.181% (2785/2808)\n",
      "Step: 400 | Loss1: 0.136 | Loss2: 0.08359 | Loss3: 0.07479 | Loss_Gen: 0.06903 |Loss_ORI: 0.15526 | Loss: 0.449 | Acc: 99.190% (3182/3208)\n",
      "Step: 0 | Loss: 0.047 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.044 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.042 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 41\n",
      "Step: 0 | Loss1: 0.061 | Loss2: 0.04006 | Loss3: 0.04167 | Loss_Gen: 0.06860 |Loss_ORI: 0.07461 | Loss: 0.217 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.133 | Loss2: 0.08879 | Loss3: 0.07203 | Loss_Gen: 0.06491 |Loss_ORI: 0.14755 | Loss: 0.442 | Acc: 99.020% (404/408)\n",
      "Step: 100 | Loss1: 0.139 | Loss2: 0.09344 | Loss3: 0.08289 | Loss_Gen: 0.06481 |Loss_ORI: 0.16616 | Loss: 0.482 | Acc: 98.762% (798/808)\n",
      "Step: 150 | Loss1: 0.142 | Loss2: 0.09563 | Loss3: 0.08518 | Loss_Gen: 0.06473 |Loss_ORI: 0.16699 | Loss: 0.490 | Acc: 98.841% (1194/1208)\n",
      "Step: 200 | Loss1: 0.141 | Loss2: 0.09446 | Loss3: 0.08435 | Loss_Gen: 0.06425 |Loss_ORI: 0.16731 | Loss: 0.487 | Acc: 98.818% (1589/1608)\n",
      "Step: 250 | Loss1: 0.138 | Loss2: 0.09205 | Loss3: 0.08276 | Loss_Gen: 0.06440 |Loss_ORI: 0.16429 | Loss: 0.477 | Acc: 99.004% (1988/2008)\n",
      "Step: 300 | Loss1: 0.136 | Loss2: 0.09017 | Loss3: 0.08162 | Loss_Gen: 0.06437 |Loss_ORI: 0.16042 | Loss: 0.469 | Acc: 99.128% (2387/2408)\n",
      "Step: 350 | Loss1: 0.135 | Loss2: 0.08911 | Loss3: 0.08065 | Loss_Gen: 0.06424 |Loss_ORI: 0.15996 | Loss: 0.465 | Acc: 98.932% (2778/2808)\n",
      "Step: 400 | Loss1: 0.134 | Loss2: 0.08732 | Loss3: 0.07906 | Loss_Gen: 0.06416 |Loss_ORI: 0.15746 | Loss: 0.457 | Acc: 99.002% (3176/3208)\n",
      "Step: 0 | Loss: 0.033 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.040 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.043 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 42\n",
      "Step: 0 | Loss1: 0.092 | Loss2: 0.05418 | Loss3: 0.04676 | Loss_Gen: 0.06107 |Loss_ORI: 0.09078 | Loss: 0.284 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.121 | Loss2: 0.07646 | Loss3: 0.06601 | Loss_Gen: 0.06290 |Loss_ORI: 0.13694 | Loss: 0.400 | Acc: 99.755% (407/408)\n",
      "Step: 100 | Loss1: 0.123 | Loss2: 0.08116 | Loss3: 0.07075 | Loss_Gen: 0.06231 |Loss_ORI: 0.14622 | Loss: 0.422 | Acc: 99.257% (802/808)\n",
      "Step: 150 | Loss1: 0.120 | Loss2: 0.07911 | Loss3: 0.06977 | Loss_Gen: 0.06279 |Loss_ORI: 0.14497 | Loss: 0.414 | Acc: 99.172% (1198/1208)\n",
      "Step: 200 | Loss1: 0.124 | Loss2: 0.08355 | Loss3: 0.07334 | Loss_Gen: 0.06271 |Loss_ORI: 0.15440 | Loss: 0.435 | Acc: 98.756% (1588/1608)\n",
      "Step: 250 | Loss1: 0.123 | Loss2: 0.08356 | Loss3: 0.07302 | Loss_Gen: 0.06279 |Loss_ORI: 0.15210 | Loss: 0.431 | Acc: 98.805% (1984/2008)\n",
      "Step: 300 | Loss1: 0.124 | Loss2: 0.08507 | Loss3: 0.07403 | Loss_Gen: 0.06289 |Loss_ORI: 0.15166 | Loss: 0.435 | Acc: 98.837% (2380/2408)\n",
      "Step: 350 | Loss1: 0.126 | Loss2: 0.08715 | Loss3: 0.07573 | Loss_Gen: 0.06295 |Loss_ORI: 0.15644 | Loss: 0.445 | Acc: 98.825% (2775/2808)\n",
      "Step: 400 | Loss1: 0.125 | Loss2: 0.08637 | Loss3: 0.07507 | Loss_Gen: 0.06284 |Loss_ORI: 0.15598 | Loss: 0.442 | Acc: 98.940% (3174/3208)\n",
      "Step: 0 | Loss: 0.019 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.039 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.038 |Combined Acc: 99.717% (705/707)\n",
      "\n",
      "Epoch: 43\n",
      "Step: 0 | Loss1: 0.074 | Loss2: 0.04586 | Loss3: 0.04412 | Loss_Gen: 0.05953 |Loss_ORI: 0.08514 | Loss: 0.249 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.114 | Loss2: 0.08177 | Loss3: 0.07142 | Loss_Gen: 0.06277 |Loss_ORI: 0.15995 | Loss: 0.428 | Acc: 98.529% (402/408)\n",
      "Step: 100 | Loss1: 0.121 | Loss2: 0.08898 | Loss3: 0.07285 | Loss_Gen: 0.06228 |Loss_ORI: 0.16046 | Loss: 0.443 | Acc: 99.010% (800/808)\n",
      "Step: 150 | Loss1: 0.115 | Loss2: 0.08274 | Loss3: 0.06947 | Loss_Gen: 0.06231 |Loss_ORI: 0.15178 | Loss: 0.419 | Acc: 99.255% (1199/1208)\n",
      "Step: 200 | Loss1: 0.113 | Loss2: 0.08055 | Loss3: 0.06801 | Loss_Gen: 0.06262 |Loss_ORI: 0.14788 | Loss: 0.409 | Acc: 99.378% (1598/1608)\n",
      "Step: 250 | Loss1: 0.113 | Loss2: 0.07854 | Loss3: 0.06676 | Loss_Gen: 0.06254 |Loss_ORI: 0.14405 | Loss: 0.402 | Acc: 99.353% (1995/2008)\n",
      "Step: 300 | Loss1: 0.114 | Loss2: 0.07797 | Loss3: 0.06857 | Loss_Gen: 0.06243 |Loss_ORI: 0.14538 | Loss: 0.406 | Acc: 99.211% (2389/2408)\n",
      "Step: 350 | Loss1: 0.115 | Loss2: 0.07869 | Loss3: 0.06927 | Loss_Gen: 0.06258 |Loss_ORI: 0.14598 | Loss: 0.409 | Acc: 99.181% (2785/2808)\n",
      "Step: 400 | Loss1: 0.116 | Loss2: 0.07895 | Loss3: 0.07068 | Loss_Gen: 0.06255 |Loss_ORI: 0.14706 | Loss: 0.413 | Acc: 99.158% (3181/3208)\n",
      "Step: 0 | Loss: 0.027 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.037 |Combined Acc: 99.720% (356/357)\n",
      "Step: 100 | Loss: 0.036 |Combined Acc: 99.859% (706/707)\n",
      "\n",
      "Epoch: 44\n",
      "Step: 0 | Loss1: 0.275 | Loss2: 0.06625 | Loss3: 0.07631 | Loss_Gen: 0.05787 |Loss_ORI: 0.13694 | Loss: 0.555 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.113 | Loss2: 0.08165 | Loss3: 0.07351 | Loss_Gen: 0.06286 |Loss_ORI: 0.14804 | Loss: 0.416 | Acc: 98.775% (403/408)\n",
      "Step: 100 | Loss1: 0.118 | Loss2: 0.08471 | Loss3: 0.07342 | Loss_Gen: 0.06239 |Loss_ORI: 0.15499 | Loss: 0.431 | Acc: 98.639% (797/808)\n",
      "Step: 150 | Loss1: 0.126 | Loss2: 0.08700 | Loss3: 0.07708 | Loss_Gen: 0.06275 |Loss_ORI: 0.16122 | Loss: 0.451 | Acc: 98.758% (1193/1208)\n",
      "Step: 200 | Loss1: 0.119 | Loss2: 0.08370 | Loss3: 0.07201 | Loss_Gen: 0.06246 |Loss_ORI: 0.15058 | Loss: 0.425 | Acc: 99.067% (1593/1608)\n",
      "Step: 250 | Loss1: 0.124 | Loss2: 0.08686 | Loss3: 0.07488 | Loss_Gen: 0.06213 |Loss_ORI: 0.15654 | Loss: 0.443 | Acc: 99.004% (1988/2008)\n",
      "Step: 300 | Loss1: 0.126 | Loss2: 0.08675 | Loss3: 0.07768 | Loss_Gen: 0.06214 |Loss_ORI: 0.15935 | Loss: 0.450 | Acc: 99.003% (2384/2408)\n",
      "Step: 350 | Loss1: 0.130 | Loss2: 0.08642 | Loss3: 0.07709 | Loss_Gen: 0.06217 |Loss_ORI: 0.15926 | Loss: 0.452 | Acc: 99.003% (2780/2808)\n",
      "Step: 400 | Loss1: 0.130 | Loss2: 0.08586 | Loss3: 0.07689 | Loss_Gen: 0.06217 |Loss_ORI: 0.15901 | Loss: 0.452 | Acc: 98.971% (3175/3208)\n",
      "Step: 0 | Loss: 0.027 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.040 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.040 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 45\n",
      "Step: 0 | Loss1: 0.071 | Loss2: 0.05560 | Loss3: 0.03415 | Loss_Gen: 0.05707 |Loss_ORI: 0.07376 | Loss: 0.234 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.119 | Loss2: 0.07174 | Loss3: 0.06960 | Loss_Gen: 0.06283 |Loss_ORI: 0.13382 | Loss: 0.394 | Acc: 99.755% (407/408)\n",
      "Step: 100 | Loss1: 0.118 | Loss2: 0.07732 | Loss3: 0.06983 | Loss_Gen: 0.06344 |Loss_ORI: 0.13614 | Loss: 0.401 | Acc: 99.629% (805/808)\n",
      "Step: 150 | Loss1: 0.122 | Loss2: 0.08034 | Loss3: 0.06955 | Loss_Gen: 0.06280 |Loss_ORI: 0.14049 | Loss: 0.412 | Acc: 99.503% (1202/1208)\n",
      "Step: 200 | Loss1: 0.125 | Loss2: 0.08241 | Loss3: 0.07378 | Loss_Gen: 0.06273 |Loss_ORI: 0.14482 | Loss: 0.426 | Acc: 99.378% (1598/1608)\n",
      "Step: 250 | Loss1: 0.123 | Loss2: 0.08128 | Loss3: 0.07365 | Loss_Gen: 0.06266 |Loss_ORI: 0.14456 | Loss: 0.422 | Acc: 99.402% (1996/2008)\n",
      "Step: 300 | Loss1: 0.122 | Loss2: 0.07985 | Loss3: 0.07313 | Loss_Gen: 0.06264 |Loss_ORI: 0.14344 | Loss: 0.418 | Acc: 99.502% (2396/2408)\n",
      "Step: 350 | Loss1: 0.123 | Loss2: 0.08198 | Loss3: 0.07426 | Loss_Gen: 0.06257 |Loss_ORI: 0.14723 | Loss: 0.427 | Acc: 99.359% (2790/2808)\n",
      "Step: 400 | Loss1: 0.123 | Loss2: 0.08167 | Loss3: 0.07506 | Loss_Gen: 0.06247 |Loss_ORI: 0.14882 | Loss: 0.429 | Acc: 99.252% (3184/3208)\n",
      "Step: 0 | Loss: 0.018 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.028 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.029 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 46\n",
      "Step: 0 | Loss1: 0.093 | Loss2: 0.06668 | Loss3: 0.06048 | Loss_Gen: 0.06230 |Loss_ORI: 0.11636 | Loss: 0.336 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.142 | Loss2: 0.09213 | Loss3: 0.07477 | Loss_Gen: 0.06267 |Loss_ORI: 0.17618 | Loss: 0.485 | Acc: 97.794% (399/408)\n",
      "Step: 100 | Loss1: 0.123 | Loss2: 0.08030 | Loss3: 0.06964 | Loss_Gen: 0.06259 |Loss_ORI: 0.15360 | Loss: 0.426 | Acc: 98.515% (796/808)\n",
      "Step: 150 | Loss1: 0.126 | Loss2: 0.08089 | Loss3: 0.06963 | Loss_Gen: 0.06257 |Loss_ORI: 0.15174 | Loss: 0.428 | Acc: 98.427% (1189/1208)\n",
      "Step: 200 | Loss1: 0.121 | Loss2: 0.07931 | Loss3: 0.06831 | Loss_Gen: 0.06238 |Loss_ORI: 0.14571 | Loss: 0.414 | Acc: 98.756% (1588/1608)\n",
      "Step: 250 | Loss1: 0.122 | Loss2: 0.08050 | Loss3: 0.06965 | Loss_Gen: 0.06221 |Loss_ORI: 0.14794 | Loss: 0.421 | Acc: 98.805% (1984/2008)\n",
      "Step: 300 | Loss1: 0.123 | Loss2: 0.08078 | Loss3: 0.07087 | Loss_Gen: 0.06201 |Loss_ORI: 0.14978 | Loss: 0.425 | Acc: 98.837% (2380/2408)\n",
      "Step: 350 | Loss1: 0.122 | Loss2: 0.08132 | Loss3: 0.07138 | Loss_Gen: 0.06207 |Loss_ORI: 0.14940 | Loss: 0.424 | Acc: 98.896% (2777/2808)\n",
      "Step: 400 | Loss1: 0.121 | Loss2: 0.08028 | Loss3: 0.07058 | Loss_Gen: 0.06210 |Loss_ORI: 0.14805 | Loss: 0.420 | Acc: 98.940% (3174/3208)\n",
      "Step: 0 | Loss: 0.015 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.039 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.038 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 47\n",
      "Step: 0 | Loss1: 0.055 | Loss2: 0.07368 | Loss3: 0.04019 | Loss_Gen: 0.06239 |Loss_ORI: 0.07143 | Loss: 0.240 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.113 | Loss2: 0.07303 | Loss3: 0.06311 | Loss_Gen: 0.06136 |Loss_ORI: 0.13300 | Loss: 0.383 | Acc: 99.510% (406/408)\n",
      "Step: 100 | Loss1: 0.126 | Loss2: 0.08148 | Loss3: 0.07516 | Loss_Gen: 0.06171 |Loss_ORI: 0.15169 | Loss: 0.434 | Acc: 98.886% (799/808)\n",
      "Step: 150 | Loss1: 0.127 | Loss2: 0.08986 | Loss3: 0.07907 | Loss_Gen: 0.06174 |Loss_ORI: 0.15202 | Loss: 0.448 | Acc: 98.758% (1193/1208)\n",
      "Step: 200 | Loss1: 0.127 | Loss2: 0.08853 | Loss3: 0.07968 | Loss_Gen: 0.06161 |Loss_ORI: 0.15777 | Loss: 0.453 | Acc: 98.694% (1587/1608)\n",
      "Step: 250 | Loss1: 0.124 | Loss2: 0.08586 | Loss3: 0.07760 | Loss_Gen: 0.06156 |Loss_ORI: 0.15417 | Loss: 0.442 | Acc: 98.755% (1983/2008)\n",
      "Step: 300 | Loss1: 0.125 | Loss2: 0.08606 | Loss3: 0.07633 | Loss_Gen: 0.06166 |Loss_ORI: 0.15357 | Loss: 0.441 | Acc: 98.713% (2377/2408)\n",
      "Step: 350 | Loss1: 0.124 | Loss2: 0.08416 | Loss3: 0.07418 | Loss_Gen: 0.06163 |Loss_ORI: 0.15012 | Loss: 0.433 | Acc: 98.789% (2774/2808)\n",
      "Step: 400 | Loss1: 0.127 | Loss2: 0.08427 | Loss3: 0.07521 | Loss_Gen: 0.06168 |Loss_ORI: 0.15077 | Loss: 0.437 | Acc: 98.909% (3173/3208)\n",
      "Step: 0 | Loss: 0.035 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.035 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.033 |Combined Acc: 100.000% (707/707)\n",
      "\n",
      "Epoch: 48\n",
      "Step: 0 | Loss1: 0.189 | Loss2: 0.07719 | Loss3: 0.07154 | Loss_Gen: 0.05686 |Loss_ORI: 0.17783 | Loss: 0.516 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.125 | Loss2: 0.07993 | Loss3: 0.07275 | Loss_Gen: 0.06141 |Loss_ORI: 0.14820 | Loss: 0.426 | Acc: 99.265% (405/408)\n",
      "Step: 100 | Loss1: 0.121 | Loss2: 0.07987 | Loss3: 0.07706 | Loss_Gen: 0.06129 |Loss_ORI: 0.14170 | Loss: 0.420 | Acc: 99.381% (803/808)\n",
      "Step: 150 | Loss1: 0.120 | Loss2: 0.07950 | Loss3: 0.07480 | Loss_Gen: 0.06125 |Loss_ORI: 0.14208 | Loss: 0.417 | Acc: 99.172% (1198/1208)\n",
      "Step: 200 | Loss1: 0.126 | Loss2: 0.08249 | Loss3: 0.07907 | Loss_Gen: 0.06132 |Loss_ORI: 0.14862 | Loss: 0.436 | Acc: 99.192% (1595/1608)\n",
      "Step: 250 | Loss1: 0.126 | Loss2: 0.08184 | Loss3: 0.07683 | Loss_Gen: 0.06137 |Loss_ORI: 0.14709 | Loss: 0.431 | Acc: 99.203% (1992/2008)\n",
      "Step: 300 | Loss1: 0.127 | Loss2: 0.08058 | Loss3: 0.07547 | Loss_Gen: 0.06153 |Loss_ORI: 0.14654 | Loss: 0.429 | Acc: 99.211% (2389/2408)\n",
      "Step: 350 | Loss1: 0.130 | Loss2: 0.08329 | Loss3: 0.07784 | Loss_Gen: 0.06152 |Loss_ORI: 0.15217 | Loss: 0.444 | Acc: 99.145% (2784/2808)\n",
      "Step: 400 | Loss1: 0.127 | Loss2: 0.08154 | Loss3: 0.07542 | Loss_Gen: 0.06164 |Loss_ORI: 0.14870 | Loss: 0.433 | Acc: 99.158% (3181/3208)\n",
      "Step: 0 | Loss: 0.192 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.047 |Combined Acc: 99.440% (355/357)\n",
      "Step: 100 | Loss: 0.040 |Combined Acc: 99.717% (705/707)\n",
      "\n",
      "Epoch: 49\n",
      "Step: 0 | Loss1: 0.072 | Loss2: 0.04395 | Loss3: 0.03559 | Loss_Gen: 0.06204 |Loss_ORI: 0.08437 | Loss: 0.236 | Acc: 100.000% (8/8)\n",
      "Step: 50 | Loss1: 0.115 | Loss2: 0.07797 | Loss3: 0.07249 | Loss_Gen: 0.06138 |Loss_ORI: 0.15426 | Loss: 0.420 | Acc: 99.510% (406/408)\n",
      "Step: 100 | Loss1: 0.128 | Loss2: 0.08225 | Loss3: 0.07502 | Loss_Gen: 0.06128 |Loss_ORI: 0.15534 | Loss: 0.440 | Acc: 99.381% (803/808)\n",
      "Step: 150 | Loss1: 0.128 | Loss2: 0.08338 | Loss3: 0.07357 | Loss_Gen: 0.06088 |Loss_ORI: 0.15591 | Loss: 0.440 | Acc: 99.421% (1201/1208)\n",
      "Step: 200 | Loss1: 0.125 | Loss2: 0.08201 | Loss3: 0.07339 | Loss_Gen: 0.06132 |Loss_ORI: 0.15519 | Loss: 0.436 | Acc: 99.192% (1595/1608)\n",
      "Step: 250 | Loss1: 0.127 | Loss2: 0.08171 | Loss3: 0.07339 | Loss_Gen: 0.06138 |Loss_ORI: 0.15298 | Loss: 0.435 | Acc: 99.253% (1993/2008)\n",
      "Step: 300 | Loss1: 0.123 | Loss2: 0.08025 | Loss3: 0.07165 | Loss_Gen: 0.06146 |Loss_ORI: 0.15011 | Loss: 0.425 | Acc: 99.294% (2391/2408)\n",
      "Step: 350 | Loss1: 0.122 | Loss2: 0.07976 | Loss3: 0.07185 | Loss_Gen: 0.06145 |Loss_ORI: 0.15119 | Loss: 0.425 | Acc: 99.181% (2785/2808)\n",
      "Step: 400 | Loss1: 0.122 | Loss2: 0.07898 | Loss3: 0.07108 | Loss_Gen: 0.06137 |Loss_ORI: 0.15084 | Loss: 0.423 | Acc: 99.158% (3181/3208)\n",
      "Step: 0 | Loss: 0.021 |Combined Acc: 100.000% (7/7)\n",
      "Step: 50 | Loss: 0.021 |Combined Acc: 100.000% (357/357)\n",
      "Step: 100 | Loss: 0.022 |Combined Acc: 100.000% (707/707)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torchvision.models\n",
    "from sam import SAM\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from vic.loss import CharbonnierLoss\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from basic_conv import *\n",
    "from model.smooth_cross_entropy import smooth_crossentropy\n",
    "from utility.bypass_bn import enable_running_stats, disable_running_stats\n",
    "\n",
    "def cosine_anneal_schedule(t, nb_epoch, lr):\n",
    "    cos_inner = np.pi * (t % (nb_epoch))\n",
    "    cos_inner /= (nb_epoch)\n",
    "    cos_out = np.cos(cos_inner) + 1\n",
    "\n",
    "    return float(lr / 2 * cos_out)\n",
    "\n",
    "def test(net, criterion, batch_size, test_path):\n",
    "    net.eval()\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    correct_com = 0\n",
    "    total = 0\n",
    "    idx = 0\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((550, 550)),\n",
    "        transforms.CenterCrop(448),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    testset = torchvision.datasets.ImageFolder(root=test_path,\n",
    "                                               transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        idx = batch_idx\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        output_1, output_2, output_3, output_ORI, map1, map2, map3 = net(inputs)\n",
    "\n",
    "        outputs_com = output_1 + output_2 + output_3 + output_ORI\n",
    "\n",
    "        loss = criterion(output_ORI, targets).mean()\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(output_ORI.data, 1)\n",
    "        _, predicted_com = torch.max(outputs_com.data, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        correct_com += predicted_com.eq(targets.data).cpu().sum()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Step: %d | Loss: %.3f |Combined Acc: %.3f%% (%d/%d)' % (\n",
    "            batch_idx, test_loss / (batch_idx + 1),\n",
    "            100. * float(correct_com) / total, correct_com, total))\n",
    "\n",
    "    test_acc_en = 100. * float(correct_com) / total\n",
    "    test_loss = test_loss / (idx + 1)\n",
    "\n",
    "    return test_acc_en, test_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Features(nn.Module):\n",
    "    def __init__(self, net_layers):\n",
    "        super(Features, self).__init__()\n",
    "        self.net_layer_0 = nn.Sequential(net_layers[0])\n",
    "        self.net_layer_1 = nn.Sequential(net_layers[1])\n",
    "        self.net_layer_2 = nn.Sequential(net_layers[2])\n",
    "        self.net_layer_3 = nn.Sequential(net_layers[3])\n",
    "        self.net_layer_4 = nn.Sequential(*net_layers[4])\n",
    "        self.net_layer_5 = nn.Sequential(*net_layers[5])\n",
    "        self.net_layer_6 = nn.Sequential(*net_layers[6])\n",
    "        self.net_layer_7 = nn.Sequential(*net_layers[7])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_layer_0(x)\n",
    "        x = self.net_layer_1(x)\n",
    "        x = self.net_layer_2(x)\n",
    "        x = self.net_layer_3(x)\n",
    "        x = self.net_layer_4(x)\n",
    "        x1 = self.net_layer_5(x)\n",
    "        x2 = self.net_layer_6(x1)\n",
    "        x3 = self.net_layer_7(x2)\n",
    "        return x1, x2, x3\n",
    "\n",
    "\n",
    "class Anti_Noise_Decoder(nn.Module):\n",
    "    def __init__(self, scale, in_channel):\n",
    "        super(Anti_Noise_Decoder, self).__init__()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        in_channel = in_channel // (scale * scale)\n",
    "\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.process = nn.Sequential(\n",
    "            nn.PixelShuffle(scale),\n",
    "            nn.Conv2d(in_channel, 256, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(16, 3, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, map):\n",
    "        return self.skip(x) + self.process(map)\n",
    "\n",
    "class Network_Wrapper(nn.Module):\n",
    "    def __init__(self, net_layers, num_class, classifier):\n",
    "        super().__init__()\n",
    "        self.Features = Features(net_layers)\n",
    "        self.classifier_pool = nn.Sequential(classifier[0])\n",
    "        self.classifier_initial = nn.Sequential(classifier[1])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=56, stride=1)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=28, stride=1)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=14, stride=1)\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            BasicConv(512, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_class)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            BasicConv(1024, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_class),\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            BasicConv(2048, 512, kernel_size=1, stride=1, padding=0, relu=True),\n",
    "            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n",
    "        )\n",
    "        self.classifier3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(512, num_class),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3 = self.Features(x)\n",
    "        map1 = x1.clone()\n",
    "        map2 = x2.clone()\n",
    "        map3 = x3.clone()\n",
    "\n",
    "        classifiers = self.classifier_pool(x3).view(x3.size(0), -1)\n",
    "        classifiers = self.classifier_initial(classifiers)\n",
    "\n",
    "        x1_ = self.conv_block1(x1)\n",
    "        x1_ = self.max_pool1(x1_)\n",
    "        x1_f = x1_.view(x1_.size(0), -1)\n",
    "        x1_c = self.classifier1(x1_f)\n",
    "\n",
    "        x2_ = self.conv_block2(x2)\n",
    "        x2_ = self.max_pool2(x2_)\n",
    "        x2_f = x2_.view(x2_.size(0), -1)\n",
    "        x2_c = self.classifier2(x2_f)\n",
    "\n",
    "\n",
    "        x3_ = self.conv_block3(x3)\n",
    "        x3_ = self.max_pool3(x3_)\n",
    "        x3_f = x3_.view(x3_.size(0), -1)\n",
    "        x3_c = self.classifier3(x3_f)\n",
    "\n",
    "        return x1_c, x2_c, x3_c, classifiers, map1, map2, map3\n",
    "\n",
    "def img_add_noise(x, transformation_seq):\n",
    "\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = x.cpu().numpy()\n",
    "    x = transformation_seq(images=x)\n",
    "    x = torch.from_numpy(x.astype(np.float32))\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def CELoss(x, y):\n",
    "    return smooth_crossentropy(x, y, smoothing=0.1)\n",
    "\n",
    "\n",
    "def train(nb_epoch, batch_size, store_name, num_class=0, start_epoch=0, data_path=''):\n",
    "\n",
    "    alpha = 1\n",
    "\n",
    "    exp_dir = store_name\n",
    "    try:\n",
    "        os.stat(exp_dir)\n",
    "    except:\n",
    "        os.makedirs(exp_dir)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(use_cuda)\n",
    "\n",
    "\n",
    "    print('==> Preparing data..')\n",
    "    \n",
    "    # data preparation\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((550, 550)),\n",
    "        transforms.RandomCrop(448, padding=8),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root=data_path+'/train', transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    \n",
    "    # Model Initialization\n",
    "    net = torchvision.models.resnet50()\n",
    "    state_dict = load_state_dict_from_url('https://download.pytorch.org/models/resnet50-19c8e357.pth')\n",
    "    net.load_state_dict(state_dict)\n",
    "    fc_features = net.fc.in_features\n",
    "    net.fc = nn.Linear(fc_features, num_class)\n",
    "\n",
    "    net_layers = list(net.children())\n",
    "    classifier = net_layers[8:10]\n",
    "    net_layers = net_layers[0:8]\n",
    "    net = Network_Wrapper(net_layers, num_class, classifier)\n",
    "\n",
    "    netp = torch.nn.DataParallel(net, device_ids=[0])\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    net.to(device)\n",
    "    decoder1 = Anti_Noise_Decoder(1, 512).to(device)\n",
    "    decoder2 = Anti_Noise_Decoder(2, 1024).to(device)\n",
    "    decoder3 = Anti_Noise_Decoder(4, 2048).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    CB_loss = CharbonnierLoss()\n",
    "\n",
    "    base_optimizer = torch.optim.SGD\n",
    "\n",
    "\n",
    "    optimizer = SAM([\n",
    "        {'params': net.classifier_initial.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block1.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier1.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block2.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier2.parameters(), 'lr': 0.002},\n",
    "        {'params': net.conv_block3.parameters(), 'lr': 0.002},\n",
    "        {'params': net.classifier3.parameters(), 'lr': 0.002},\n",
    "\n",
    "        {'params': decoder1.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder1.process.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder2.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder2.process.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder3.skip.parameters(), 'lr': 0.002},\n",
    "        {'params': decoder3.process.parameters(), 'lr': 0.002},\n",
    "\n",
    "        {'params': net.Features.parameters(), 'lr': 0.0002}\n",
    "\n",
    "    ],\n",
    "        base_optimizer, adaptive=False, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "    max_val_acc = 0\n",
    "    lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002,  0.002, 0.002,  0.002, 0.002,  0.002, 0.002, 0.0002]\n",
    "    flag = True\n",
    "    if start_epoch > 0:\n",
    "        # Load the checkpoint\n",
    "        if flag:\n",
    "            checkpoint = torch.load('/kaggle/input/pmal-checkpoint-model/' + store_name + '/checkpoint.pth')\n",
    "    \n",
    "            # Load states into the model and decoders\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            decoder1.load_state_dict(checkpoint['decoder1_state_dict'])\n",
    "            decoder2.load_state_dict(checkpoint['decoder2_state_dict'])\n",
    "            decoder3.load_state_dict(checkpoint['decoder3_state_dict'])\n",
    "    \n",
    "            # Load metadata\n",
    "            start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "            max_val_acc = checkpoint['max_val_acc']\n",
    "            flag = False\n",
    "    if not flag:\n",
    "        print(type(checkpoint))\n",
    "        print(type(decoder1))\n",
    "        print(start_epoch)\n",
    "        print(max_val_acc)\n",
    "        \n",
    "    \n",
    "        \n",
    "    for epoch in range(start_epoch, nb_epoch):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_loss1 = 0\n",
    "        train_loss2 = 0\n",
    "        train_loss3 = 0\n",
    "        train_loss4 = 0\n",
    "        train_loss5 = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        idx = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            idx = batch_idx\n",
    "            if inputs.shape[0] < batch_size:\n",
    "                continue\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "\n",
    "\n",
    "            for nlr in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, nb_epoch, lr[nlr])\n",
    "\n",
    "            sometimes_1 = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "            sometimes_2 = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "\n",
    "\n",
    "            trans_seq_aug = iaa.Sequential(\n",
    "                [\n",
    "\n",
    "                    sometimes_1(iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-15, 15),\n",
    "                        shear=(-15, 15),\n",
    "                        order=[0, 1],\n",
    "                        cval=(0, 1),\n",
    "                        mode=ia.ALL\n",
    "                    )),\n",
    "                    sometimes_2(iaa.GaussianBlur((0, 3.0)))\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "\n",
    "            trans_seq = iaa.Sequential(\n",
    "                [\n",
    "\n",
    "                    iaa.AdditiveGaussianNoise(\n",
    "                        loc=0, scale=(0.0, 0.05), per_channel=0.5\n",
    "                    )\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "\n",
    "            # H1\n",
    "            # H1 first forward-backward step\n",
    "            enable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "            inputs1_gt = img_add_noise(inputs, trans_seq_aug).to(device)\n",
    "            inputs1 = img_add_noise(inputs1_gt, trans_seq).to(device)\n",
    "            output_1, _, _, _, map1, _, _ = netp(inputs1)\n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "\n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "\n",
    "            output_1_syn, _, _, _, _, _, _ = netp(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "\n",
    "            loss1 = loss1_c + (alpha * loss1_g) + loss1_c_syn\n",
    "            loss1.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "\n",
    "            # H1 second forward-backward step\n",
    "            disable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_1, _, _, _, map1, _, _ = netp(inputs1)\n",
    "            loss1_c = CELoss(output_1, targets).mean() * 1\n",
    "\n",
    "            inputs1_syn = decoder1(inputs1, map1)\n",
    "            loss1_g = CB_loss(inputs1_syn, inputs1_gt) * 1\n",
    "\n",
    "            output_1_syn, _, _, _, _, _, _ = netp(inputs1_syn)\n",
    "            loss1_c_syn = CELoss(output_1_syn, targets).mean() * 1\n",
    "\n",
    "            loss1_ = loss1_c + (alpha * loss1_g) + loss1_c_syn\n",
    "            loss1_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "\n",
    "            # H2\n",
    "            # H2 first forward-backward step\n",
    "            enable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "            inputs2_gt = img_add_noise(inputs, trans_seq_aug).to(device)\n",
    "            inputs2 = img_add_noise(inputs2_gt, trans_seq).to(device)\n",
    "            _, output_2, _, _, _, map2, _ = netp(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "\n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "\n",
    "            _, output_2_syn, _, _, _, _, _ = netp(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "\n",
    "            loss2 = loss2_c + (alpha * loss2_g) + loss2_c_syn\n",
    "            loss2.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            # H2 second forward-backward step\n",
    "            disable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "            _, output_2, _, _, _, map2, _ = netp(inputs2)\n",
    "            loss2_c = CELoss(output_2, targets).mean() * 1\n",
    "\n",
    "            inputs2_syn = decoder2(inputs2, map2)\n",
    "            loss2_g = CB_loss(inputs2_syn, inputs2_gt) * 1\n",
    "\n",
    "            _, output_2_syn, _, _, _, _, _ = netp(inputs2_syn)\n",
    "            loss2_c_syn = CELoss(output_2_syn, targets).mean() * 1\n",
    "\n",
    "            loss2_ = loss2_c + (alpha * loss2_g) + loss2_c_syn\n",
    "            loss2_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            #H3\n",
    "            # H3 first forward-backward step\n",
    "            enable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "            inputs3_gt = img_add_noise(inputs, trans_seq_aug).to(device)\n",
    "            inputs3 = img_add_noise(inputs3_gt, trans_seq).to(device)\n",
    "            _, _, output_3, _, _, _, map3 = netp(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "\n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "\n",
    "            _, _, output_3_syn, _, _, _, _ = netp(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "\n",
    "            loss3 = loss3_c + (alpha * loss3_g) + loss3_c_syn\n",
    "            loss3.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            # H3 second forward-backward step\n",
    "            disable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "            _, _, output_3, _, _, _, map3 = netp(inputs3)\n",
    "            loss3_c = CELoss(output_3, targets).mean() * 1\n",
    "\n",
    "            inputs3_syn = decoder3(inputs3, map3)\n",
    "            loss3_g = CB_loss(inputs3_syn, inputs3_gt) * 1\n",
    "\n",
    "            _, _, output_3_syn, _, _, _, _ = netp(inputs3_syn)\n",
    "            loss3_c_syn = CELoss(output_3_syn, targets).mean() * 1\n",
    "\n",
    "            loss3_ = loss3_c + (alpha * loss3_g) + loss3_c_syn\n",
    "            loss3_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "            # H4\n",
    "            # H4 first forward-backward step\n",
    "            enable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = netp(inputs)\n",
    "            ORI_loss = CELoss(output_1_final, targets).mean() + \\\n",
    "                                CELoss(output_2_final, targets).mean() + \\\n",
    "                                CELoss(output_3_final, targets).mean() + \\\n",
    "                                CELoss(output_ORI, targets).mean() * 2\n",
    "            ORI_loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            # H4 second forward-backward step\n",
    "            disable_running_stats(netp)\n",
    "            optimizer.zero_grad()\n",
    "            output_1_final, output_2_final, output_3_final, output_ORI, _, _, _ = netp(inputs)\n",
    "            ORI_loss_ = CELoss(output_1_final, targets).mean() + \\\n",
    "                          CELoss(output_2_final, targets).mean() + \\\n",
    "                          CELoss(output_3_final, targets).mean() + \\\n",
    "                          CELoss(output_ORI, targets).mean() * 2\n",
    "            ORI_loss_.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(output_ORI.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            train_loss += (loss1.item() + loss2.item() + loss3.item() + ORI_loss.item())\n",
    "            train_loss1 += loss1.item()\n",
    "            train_loss2 += loss2.item()\n",
    "            train_loss3 += loss3.item()\n",
    "            train_loss4 += (loss1_g.item() + loss2_g.item() + loss3_g.item())\n",
    "            train_loss5 += ORI_loss.item()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    'Step: %d | Loss1: %.3f | Loss2: %.5f | Loss3: %.5f | Loss_Gen: %.5f |Loss_ORI: %.5f | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (\n",
    "                    batch_idx, train_loss1 / (batch_idx + 1), train_loss2 / (batch_idx + 1),\n",
    "                    train_loss3 / (batch_idx + 1), train_loss4 / (batch_idx + 1),  train_loss5/ (batch_idx + 1), train_loss / (batch_idx + 1),\n",
    "                    100. * float(correct) / total, correct, total))\n",
    "\n",
    "        train_acc = 100. * float(correct) / total\n",
    "        train_loss = train_loss / (idx + 1)\n",
    "        with open(exp_dir + '/results_train.txt', 'a') as file:\n",
    "            file.write(\n",
    "                'Iteration %d | train_acc = %.5f | train_loss = %.5f | Loss1: %.3f | Loss2: %.5f | Loss3: %.5f | Loss_Gen: %.5f | Loss_ORI: %.5f |\\n' % (\n",
    "                epoch, train_acc, train_loss, train_loss1 / (idx + 1), train_loss2 / (idx + 1), train_loss3 / (idx + 1),\n",
    "                train_loss4 / (idx + 1), train_loss5 / (idx + 1)))\n",
    "\n",
    "\n",
    "        val_acc_com, val_loss = test(net, CELoss, 7, data_path+'/test')\n",
    "        if val_acc_com > max_val_acc:\n",
    "            max_val_acc = val_acc_com\n",
    "            net.cpu()\n",
    "            decoder1.cpu()\n",
    "            decoder2.cpu()\n",
    "            decoder3.cpu()\n",
    "            torch.save(net, './' + store_name + '/model.pth')\n",
    "            torch.save(decoder1, './' + store_name + '/decoder1.pth')\n",
    "            torch.save(decoder2, './' + store_name + '/decoder2.pth')\n",
    "            torch.save(decoder3, './' + store_name + '/decoder3.pth')\n",
    "            net.to(device)\n",
    "            decoder1.to(device)\n",
    "            decoder2.to(device)\n",
    "            decoder3.to(device)\n",
    "        with open(exp_dir + '/results_test.txt', 'a') as file:\n",
    "            file.write('Iteration %d, test_acc_combined = %.5f, test_loss = %.6f\\n' % (\n",
    "            epoch, val_acc_com, val_loss))\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'max_val_acc': max_val_acc,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'decoder1_state_dict': decoder1.state_dict(),\n",
    "            'decoder2_state_dict': decoder2.state_dict(),\n",
    "            'decoder3_state_dict': decoder3.state_dict(),\n",
    "        }\n",
    "\n",
    "        # Save the checkpoint\n",
    "        torch.save(checkpoint, './' + store_name + '/checkpoint.pth')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = '/kaggle/input/anti-noise-fgvr-v2/dataset_folder'\n",
    "    if not os.path.isdir('results'):\n",
    "        os.mkdir('results')\n",
    "    train(nb_epoch=50,             # number of epoch\n",
    "             batch_size=8,         # batch size\n",
    "             store_name='results/Stanford_Cars_ResNet50_PMAL',     # the folder for saving results\n",
    "             num_class=5,          # number of categories\n",
    "             start_epoch=25,         # the start epoch number\n",
    "             data_path = data_path)   # the path to the dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6106458,
     "sourceId": 9933503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6124107,
     "sourceId": 9957306,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40588.571248,
   "end_time": "2024-11-20T17:23:12.455597",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-20T06:06:43.884349",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
